---
title: "VoiceflowのAIアシスト機能を試してみた①Generative Tasks"
emoji: "🗣️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["Voiceflow","Voice","LLM"]
published: false
---

## 「AIアシスト機能」とは？

スマートスピーカーやチャットボットなどの会話フローをGUIで作成できるサービス[Voiceflow](https://voiceflow.com/)に新しく「AIアシスト機能」がリリースされました。

AIアシスト機能は、かんたんにいうとLLM(Large Language Model)による会話モデル設計を支援する機能です。以下の2つの機能があります。

- Generative Tasks
- Freestyle

今回はGenerative Tasksについて試してみます。

:::message
AIアシスト機能はベータリリースです。機能の内容等については今後変更される可能性があります。予めご留意ください。
:::

:::message
LLMはおそらくGPT-3のようです。
https://twitter.com/VoiceflowHQ/status/1615444628349779968
:::

### プロジェクトの作成

何はともあれ、まずはプロジェクトを作成しましよう。

![create assistant](/images/20230125_01.png =400x)

プロジェクト名は適当に設定してください。プロジェクトタイプについてですが、現時点で確認した限り、プロジェクトタイプによってAIアシスト機能（の一部）が使えないようです。今回は私の方で事前に「フル」に使えることを確認している"Web Chatbot"タイプにします。"Launch & host"を選択して"Continue"をクリックします。

![create assistant](/images/20230125_02.png =400x)

チャンネルに"Web Chat"、言語は"Japanese(ja-JP)"を選択して、"Create Assistant"をクリックします。

![create assistant](/images/20230125_03.png =400x)

プロジェクトが作成されました。ここから会話フローを作成しつつ、AIアシスト機能でできることを1つづつ試していきます。

![canvas](/images/20230125_08.png)

### トークンについて

AIアシスト機能について説明する前に、最初に「トークン」について説明しておきます。

AIアシスト機能の使用は「トークン」という単位で管理され、AIアシスタント機能を使うたびにカウントされます。画面の左下のアイコンを見てください。

![canvas](/images/20230125_04.png)

ここにカーソルをあわせると、各プランごとに割り当てられたトークンの上限と現在の使用量が確認できます。上限を超えるとAIアシスト機能が使えなくなるのではないかと思われます。

![canvas](/images/20230125_05.png =400x)

現時点で、AIアシスト機能はベータサービスとして無料で全Voiceflowユーザに提供されていますが、将来的には各プランごとのトークン上限の見直しや有償でのトークン追加なども検討されているようです。

:::message
アイコンが"On"になっていれば、そのプロジェクトでGenerative Tasks機能が有効化されています。現時点で全てのプロジェクトでGenerative Tasks機能は有効化されているようです。
:::

:::message
トークンについての詳細は以下をご覧ください。
https://voiceflow.zendesk.com/hc/en-us/articles/11696284619533-Request-more-Tokens
:::

### Generative Tasksでできること

では実際に使ってみましょう。

**Generative Tasks**は、会話モデルの設計において必要となる以下のタスクをAIによる自動生成で支援します。

- レスポンスのバリエーションの生成
- サンプル発話の生成
- スロット/エンティティ値やシノニムの生成

実際に会話モデルを作成しながら試してみましょう。最初に"Welcome"と書かれたブロックをクリックします。

![canvas](/images/20230125_06.png)

"Welcome"では、会話フローの一番最初にボットが返すレスポンスを記載します。今回はサンプルとしてよくある「コーヒーショップ」の会話にしてみました。

![canvas](/images/20230125_07.png)

これにより、この会話フローを開始すると必ず「コーヒーショップにようこそ。ご注文は何にしますか？」とボットがレスポンスするようになりますが、毎回同じレスポンスだといかにも機械的ですよね。レスポンスにバリエーションをつけたいところです。

そこで"Generate"にマウスカーソルを合わせてみてください。

![canvas](/images/20230125_09.png)

"variants"はバリエーションのことです。"Generate 3 variants"をクリックしてみましょう。

![canvas](/images/20230125_10.png)

このように最初に入力したレスポンスの内容を元に、似たような表現を指定した数だけ自動で生成してくれます。日本語的に少し表現がおかしいものも生成されたりしますが、その場合は削除（右側の"ー"をクリック）して再度"Generate"から生成するか、手動で直接修正すればよいです。また、最初に述べたとおり、生成される内容は最初に入力した内容を元に生成されるので、なるべく多くの情報を記載することでより精度の高いレスポンスが生成されるのではないかと思われます。

![canvas](/images/20230125_11.png =450x)

次にユーザからの発話を受け取るインテントを作ってみます。左のメニューから"Choice"を"Welcome"の発話の下に追加します。

![canvas](/images/20230125_12.png)

飲み物の注文を受け取るようなインテントを作ってみましょう。インテント名を"order_drink"と入力して"+"をクリックします。

![canvas](/images/20230125_13.png)

インテントに紐づくサンプル発話(Utteraces)を設定します。ユーザーの発話は本当にいろいろなバリエーションがいろいろあります。サンプル発話を多く登録することで幅広く対応できるのでなるべくたくさん登録しておきたいところですね。ここでも"Generate"が表示されていますね。元となるサンプル発話を入力せずに"Generate 5 variants"をクリックしてみましょう。

![canvas](/images/20230125_14.png =400x)

レスポンスのバリエーションと同様に、サンプル発話が5つ自動で生成されましたが、今回は元となるサンプル発話を入力していませんでしたよね。サンプル発話がなくてもインテント名から類推して生成しているのだと思われます。

![canvas](/images/20230125_15.png =400x)

ただし、インテント名だけだと情報が足りないのか少し使いにくい感じのサンプル発話になっていますよね。そういう場合はいくつかサンプル発話を入力してから"Generate"するとよいでしょう。

![canvas](/images/20230125_16.png =400x)

![canvas](/images/20230125_17.png =400x)

サンプル発話ができたら、次はスロット/エンティティ（以後、エンティティで統一します。）です。ビルトインではなくカスタムなエンティティを作りましょう。サンプル発話の入力欄に"{drink}"と入力して"+"をクリックします。

![canvas](/images/20230125_18.png =400x)

ここでもエンティティの値を入力せずに"Generate"してみましょう。

![canvas](/images/20230125_19.png =400x)

このように、エンティティの値だけでなくシノニム（同義語）もあわせて生成されます。少しおかしなものもありますが、サンプル発話と同様にいくつか入力してから生成するか、不要なものは削除すると良いでしょう。問題なければ"Create Entity"でエンティティを作成します。

![canvas](/images/20230125_20.png =400x)

インテントの画面に戻って、エンティティを含む形でサンプル発話をいくつか入力して再度"Generate"すると、自動生成されたサンプル発話もきちんとエンティティを含んだ形で生成されます。

![canvas](/images/20230125_21.png =400x)

また用意しておいたインテントのどれにもマッチせずにfallbackした場合は"No Match"を使いますが、ここでもレスポンスのバリエーションを自動で生成できます。

![canvas](/images/20230125_22.png)

![canvas](/images/20230125_23.png)

## まとめ

会話型インターフェースにおいて、ユーザからの発話、ボットのレスポンス、つまりインタラクションの入出力の両方で「バリエーション」はとても重要です。

>**さまざまな発話をインテントに一致させる**
>
>(略)
>
>あるインテントに対して、開発者が想定したとおりの文言をユーザーが言うとは限りません。ユーザーは「旅行の計画を立てて」と言うこともあるでしょうし、「休みにハワイに行きたい」と言う可能性もあります。 ユーザーのさまざまな発話に対応できるよう、ユーザーが言いそうな文、フレーズ、言葉を想定し、幅広く用意しておきます。
>
https://developer.amazon.com/ja-JP/docs/alexa/alexa-design/relatable.html#vary-alexas-responses-in-repetitive-tasks

>**繰り返しのタスクではAlexaの応答に変化をつける**
>
>ユーザーはAlexaとかなり頻繁にやり取りします。このため、よく使う対話や同じやり取りが繰り返される場合では、応答に変化をつけることをお勧めします。具体的には、談話標識（文と文を区切る言葉、たとえば「えーと」や「はい」）と、エラーが何度も繰り返される場合のプロンプト（低信頼度、無言）において実践するのがお勧めです。変化をつけた応答をランダムに選択し、Alexaがロボットのように聞こえないようにします。

https://developer.amazon.com/ja-JP/docs/alexa/alexa-design/relatable.html#vary-alexas-responses-in-repetitive-tasks

ただ、このバリエーションを用意する作業、単純な割にめちゃめちゃ工数かかるんですよね、しかも作って終わりではなく、実際の使用結果からも継続的にメンテナンスが必要になります。物量的にもそうですし、1人で考えるのにも限界があります。

LLMのメリットを活かしたAIアシスト機能を使うことで、こういった単純作業はAIにおまかせして、よりスムーズで自然な会話のデザインに注力できるのではないでしょうか。ぜひ試してみてください。

https://twitter.com/aiminakajima/status/1611256327581597697

AIアシスト機能のもうひとつ、Freestyleについては別の記事で書く予定です。
